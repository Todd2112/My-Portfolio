safety copy 12/20/2025

# my_coder.py
# Block 1.0: Configuration and Setup (Vault-Strict & Fine-Tuned)
# CLDK-VIBE: Local, multi-agent, self-learning coding assistant

import os
import re
import ast
import json
import time
import threading
import datetime
import subprocess
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple

import requests
from flask import Flask, request, jsonify, render_template, Response

# === Block 1.0: Configuration and Setup ===

# Optional Dependency Check
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False

# 1.1 The Vault (Single Source of Truth)
BASE_DIR = Path(__file__).parent
VAULT_DIR = Path(r"C:\Projects\ai-train\.vibe_index")
VAULT_DIR.mkdir(exist_ok=True, parents=True)

# 1.2 Fine-Tuned LLM Parameters
CODING_BRAIN = os.getenv("CODING_BRAIN", "codellama:7b")
REASONING_BRAIN = os.getenv("REASONING_BRAIN", "llama3.2:3b-instruct-q8_0")
ORGANIZER_BRAIN = os.getenv("ORGANIZER_BRAIN", "llama3.2:1b-instruct-q4_K_M")
EMBEDDING_MODEL = "nomic-embed-text"
MAX_FEATURE_HISTORY = 10

OLLAMA_API = "http://localhost:11434/api/generate"
OLLAMA_EMBEDDING_API = "http://localhost:11434/api/embeddings"

# 1.3 PowerShell Validation (Existing)
ENABLE_PS_VALIDATION = os.getenv("ENABLE_PS_VALIDATION", "true").lower() == "true"
PS_TIMEOUT = int(os.getenv("PS_TIMEOUT", "30"))

# --- ADD THESE TWO LINES HERE TO FIX THE ERRORS ---
BRAIN_CONFIG = {"coding": {"model": CODING_BRAIN}, "reasoning": {"model": REASONING_BRAIN}, "organizer": {"model": ORGANIZER_BRAIN}}
FEATURE_LIST_FILE = VAULT_DIR / "feature_list.json"
# --------------------------------------------------

# 1.4 Persistent Memory Artifacts
VAULT_PATHS = {
    "rag_index": VAULT_DIR / "rag_index.npy",
    "rag_metadata": VAULT_DIR / "rag_metadata.json",
    "reasoning_profile": VAULT_DIR / "reasoning_profile.json",
    "learning_log": VAULT_DIR / "learning_log.json",
    "current_state": VAULT_DIR / "current_state.json",
    "feature_list": FEATURE_LIST_FILE,  # Reference the new alias here
    "constitution": VAULT_DIR / "vibe_memory.json"
}
MEMORY_LOCK = threading.Lock()
RAG_SIMILARITY_THRESHOLD = 0.5

# 1.5 Global Logging
def log(msg: str, level: str = "INFO"):
    print(f"[{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}][{level}] {msg}")

def count_tokens(text: str) -> int:
    """
    Rough token estimation: 1 token ≈ 4 characters for English.
    For precise counting, use tiktoken library (optional dependency).
    """
    return len(text) // 4


def log_metrics(brain: str, prompt_tokens: int, response_tokens: int, duration: float, status: str = "OK"):
    """Logs LLM call metrics for performance analysis."""
    log(
        f"[METRICS] {brain.upper()} | Prompt: {prompt_tokens}t | Response: {response_tokens}t | "
        f"{duration:.2f}s | {status}",
        "METRICS"
    )

# 1.6 Global Instances
app = Flask(__name__, template_folder=str(BASE_DIR / 'templates'))
conversations: Dict[str, Dict[str, Any]] = {}

llm_manager: Optional['MultiLLMManager'] = None
rag_engine: Optional['RAGEngine'] = None
analyzer: Optional['PythonAnalyzer'] = None
cag_memory: Optional['CAGMemory'] = None
cag_pipeline: Optional['CAGPipeline'] = None
feature_manager: Optional['FeatureListManager'] = None

# === Block 2.0: Core State Management Helpers (Vault-Strict) ===

def load_state() -> Dict[str, Dict[str, Any]]:
    """Loads raw conversation state from the Vault."""
    path = VAULT_PATHS["current_state"]
    try:
        content = path.read_text(encoding='utf-8') if path.exists() else "{}"
        return json.loads(content)
    except json.JSONDecodeError:
        log("Corrupt state file in Vault, starting fresh.", "WARN")
    return {}

def load_clean_state() -> Dict[str, Dict[str, Any]]:
    """Loads state, retaining only accepted work to prevent hallucination pollution."""
    raw_state = load_state()
    clean_state = {}
    for sid, conv in raw_state.items():
        clean_state[sid] = {
            "current_code": conv.get("current_code", ""),
            "history": [m for m in conv.get("history", []) if m.get("accepted")],
            "scratchpad": {}
        }
    return clean_state

def ensure_memory_files():
    """Initializes Vault artifacts if missing without overwriting merged profiles."""
    log("Verifying Vault integrity...")
    
    # Check for Constitution (vibe_memory.json) - Critical for Hierarchy
    if not VAULT_PATHS["constitution"].exists():
        log("Constitution missing! Creating default structural rules.", "WARN")
        # Note: In a production 'Maverick' run, we'd want to stop here, 
        # but we'll provide a basic template if it's truly gone.
        default_const = {"owner": "Todd", "project_defaults": {"block_hierarchy": ["IMPORTS", "UI"]}}
        VAULT_PATHS["constitution"].write_text(json.dumps(default_const, indent=4))

    # Check for Feature List (A002 Tracking)
    if not VAULT_PATHS["feature_list"].exists():
        log(f"Creating fresh Feature List: {VAULT_PATHS['feature_list']}")
        initial_features = {
            "features": [
                {"id": "A001", "goal": "Initial Setup", "status": "passing", "history": []},
                {"id": "A002", "goal": "FeatureListManager Governance", "status": "new", "history": []}
            ]
        }
        VAULT_PATHS["feature_list"].write_text(json.dumps(initial_features, indent=4))

    # Ensure Learning Log is an empty list if missing (Clean Slate)
    if not VAULT_PATHS["learning_log"].exists():
        VAULT_PATHS["learning_log"].write_text('[]')

    log("Vault integrity check complete.", "DEBUG")

def save_state():
    """Saves the current conversation state to the Vault."""
    global conversations
    with MEMORY_LOCK:
        try:
            VAULT_PATHS["current_state"].write_text(json.dumps(conversations, indent=2))
        except Exception as e:
            log(f"Vault Save Error (State): {e}", "ERROR")

def save_learning_log(log_list: List[Dict]):
    """Saves learned patterns to the Vault."""
    with MEMORY_LOCK:
        try:
            # Keep only last 100 relevant lessons
            VAULT_PATHS["learning_log"].write_text(json.dumps(log_list[-100:], indent=2))
        except Exception as e:
            log(f"Vault Save Error (Log): {e}", "ERROR")

def load_learning_log() -> List[Dict]:
    """Loads lessons from the Vault."""
    path = VAULT_PATHS["learning_log"]
    try:
        return json.loads(path.read_text()) if path.exists() else []
    except Exception:
        return []

def load_reasoning_profile() -> Dict:
    """Loads the Precision Reasoner profile from the Vault."""
    path = VAULT_PATHS["reasoning_profile"]
    if path.exists():
        try:
            return json.loads(path.read_text())
        except json.JSONDecodeError:
            log("Vault Profile corrupt! Check reasoning_profile.json", "ERROR")
    return {} # Return empty to trigger fallback logic in LLM Manager

def get_conversation() -> Dict[str, Any]:
    """Gets local conversation state."""
    sid = "local-user" 
    with MEMORY_LOCK:
        if sid not in conversations:
            conversations[sid] = {
                "current_code": "",
                "history": [],
                "last_review": None,
                "scratchpad": {},
                "current_feature_id": None, 
            }
        return conversations[sid]

def save_message(conv: Dict, role: str, content: str, code: Optional[str] = None, accepted: bool = False):
    """Saves message with timestamp and acceptance flag."""
    conv["history"].append({
        "role": role,
        "content": content,
        "code": code,
        "timestamp": time.time(),
        "accepted": accepted
    })
    conv["history"] = conv["history"][-20:] # Increased history for better context

def extract_code(text: str) -> str:
    """Extracts code blocks, prioritizing the last block as the final output."""
    matches = re.findall(r"```(?:[a-zA-Z0-9]+)?\s*([\s\S]+?)\s*```", text, re.DOTALL)
    if matches:
        return matches[-1].strip()
    return "" if not re.search(r"(def |class |import )", text) else text.strip()

def detect_hallucinations(generated_code: str, original_code: str = "") -> Dict[str, Any]:
    """
    Multi-layer hallucination detection:
    1. Syntax validity
    2. Placeholder detection
    3. Function preservation (if modifying existing code)
    4. Import preservation
    5. Confidence scoring
    """
    report = {
        "hallucination_detected": False,
        "confidence": 1.0,
        "issues": [],
        "safe_to_persist": True
    }
    
    # Layer 1: Syntax Check
    try:
        ast.parse(generated_code)
    except SyntaxError as e:
        report["hallucination_detected"] = True
        report["confidence"] = 0.0
        report["issues"].append(f"Invalid syntax: {e.msg}")
        report["safe_to_persist"] = False
        return report  # Fatal, stop here
    
    # Layer 2: Placeholder Detection
    placeholder_patterns = [
        r"TODO", r"FIXME", r"XXX", r"HACK",
        r"placeholder", r"not implemented",
        r"pass\s*#", r"...\s*#"  # Ellipsis with comment
    ]
    for pattern in placeholder_patterns:
        if re.search(pattern, generated_code, re.IGNORECASE):
            report["hallucination_detected"] = True
            report["confidence"] -= 0.3
            report["issues"].append(f"Placeholder detected: {pattern}")
    
    # Layer 3: Empty or Trivial Implementations
    if generated_code.strip() == "pass":
        report["hallucination_detected"] = True
        report["confidence"] = 0.0
        report["issues"].append("Trivial implementation: Only 'pass' statement")
    
    # Layer 4: Function Preservation (if modifying existing code)
    if original_code:
        try:
            orig_tree = ast.parse(original_code)
            new_tree = ast.parse(generated_code)
            
            orig_funcs = {n.name for n in ast.walk(orig_tree) if isinstance(n, ast.FunctionDef)}
            new_funcs = {n.name for n in ast.walk(new_tree) if isinstance(n, ast.FunctionDef)}
            
            missing_funcs = orig_funcs - new_funcs
            if missing_funcs:
                report["hallucination_detected"] = True
                report["confidence"] -= 0.4
                report["issues"].append(f"Missing functions: {missing_funcs}")
        except:
            pass  # Skip if parsing fails
    
    # Layer 5: Import Consistency
    if original_code and "import " in original_code:
        orig_imports = set(re.findall(r"^import\s+(\w+)", original_code, re.MULTILINE))
        orig_imports.update(re.findall(r"^from\s+(\w+)", original_code, re.MULTILINE))
        
        new_imports = set(re.findall(r"^import\s+(\w+)", generated_code, re.MULTILINE))
        new_imports.update(re.findall(r"^from\s+(\w+)", generated_code, re.MULTILINE))
        
        missing_imports = orig_imports - new_imports
        if missing_imports:
            report["confidence"] -= 0.2
            report["issues"].append(f"Missing imports: {missing_imports}")
    
    # Final Decision
    report["confidence"] = max(0.0, report["confidence"])
    report["safe_to_persist"] = report["confidence"] >= 0.6 and not report["hallucination_detected"]
    
    return report

# === Block 3.0: Domain Memory Governance (Vault-Strict) ===

class FeatureListManager:
    def __init__(self):
        self.feature_file = VAULT_PATHS["feature_list"]
        self.data = self._load()

    def _load(self) -> Dict[str, Any]:
        try:
            if self.feature_file.exists():
                return json.loads(self.feature_file.read_text(encoding='utf-8'))
        except Exception as e:
            log(f"Critical: Feature list load failed: {e}", "ERROR")
        return {"features": []}

    def save(self):
        with MEMORY_LOCK:
            try:
                self.feature_file.write_text(
                    json.dumps(self.data, indent=4, ensure_ascii=False), 
                    encoding='utf-8'
                )
            except Exception as e:
                log(f"Vault Write Error (Features): {e}", "ERROR")

    def get_next_failing_feature(self) -> Optional[Dict]:
        for feature in self.data.get("features", []):
            if feature.get("status", "").lower() in ["new", "failing"]:
                return feature
        return None

    def get_feature_by_id(self, feature_id: str) -> Optional[Dict]:
        for feature in self.data.get("features", []):
            if feature.get("id") == feature_id:
                return feature
        return None

    def update_status(self, feature_id: str, new_status: str, notes: str = "") -> bool:
        for feature in self.data.get("features", []):
            if feature.get("id") == feature_id:
                old_status = feature.get("status", "unknown")
                if old_status.lower() == new_status.lower():
                    return True

                feature["status"] = new_status
                ts = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                feature["history"].append({"timestamp": ts, "note": f"{old_status} -> {new_status}. {notes}"})
                feature["history"] = feature["history"][-MAX_FEATURE_HISTORY:]
                self.save()
                log(f"Feature {feature_id} updated: {new_status}")
                return True
        
        log(f"Traceability Error: Feature ID '{feature_id}' not found in Vault", "WARN")
        return False

def get_current_feature_state() -> Optional[Dict]:
    """
    MEMORY QUARANTINE FIX: Only returns currently locked features.
    Does NOT auto-lock on idle chat.
    """
    global feature_manager
    if not feature_manager:
        return None

    conv = get_conversation()
    feature_id = conv.get("current_feature_id")

    if feature_id:
        feature = feature_manager.get_feature_by_id(feature_id)
        if feature and feature["status"].lower() in ["new", "failing"]:
            log(f"Resuming locked feature: {feature_id}", "DEBUG")
            return feature
        # Feature completed or invalid - clear it
        conv["current_feature_id"] = None

    # CRITICAL: Do NOT auto-lock during idle chat
    return None

class PythonAnalyzer:
    """
    In-memory Python code analyzer and Auditor.
    Enforces Maverick Constitution (Block Hierarchy & Forbidden Sources).
    Builds a symbol table for deterministic RAG retrieval.
    """
    def __init__(self):
        self._symbol_table: Dict[str, Dict] = {}
        # FIX: Directly use the global VAULT_PATHS dictionary instead of the undefined 'vault'
        try:
            path = VAULT_PATHS["constitution"]
            if path.exists():
                self.constitution = json.loads(path.read_text(encoding='utf-8'))
                self.defaults = self.constitution.get("project_defaults", {})
            else:
                self.defaults = {}
        except Exception as e:
            log(f"Analyzer running without Governance Audit: {e}", "WARN")
            self.defaults = {}

    def analyze_in_memory(self, code_text: str, filename: str = "memory_code.py") -> Dict[str, Any]:
        audit_results = {"valid_syntax": False, "violations": []}
        
        if not code_text.strip():
            self._symbol_table = {}
            return audit_results

        self._symbol_table = {}
        try:
            tree = ast.parse(code_text, filename=filename)
            audit_results["valid_syntax"] = True
            
            # 1. Attach parents and extract symbols
            self._attach_parents(tree)
            self._extract_symbols(filename, tree, code_text)
            
            # 2. Maverick Governance Audit
            audit_results["violations"] = self._run_audit(code_text)
            
            log(f"Analyzed {len(self._symbol_table)} symbols.", "DEBUG")
        except Exception as e:
            log(f"Analyzer Parse Error: {e}", "WARN")
            audit_results["violations"].append(f"Syntax Error: {str(e)}")

        return audit_results

    def _run_audit(self, code: str) -> List[str]:
        violations = []
        if not self.defaults:
            return violations
        
        # Check Forbidden Sources (e.g., specific libraries or keywords)
        forbidden = self.defaults.get("forbidden_sources", [])
        for item in forbidden:
            if item in code:
                violations.append(f"Violation: Forbidden source detected: {item}")

        # Check Block Hierarchy (Must be sequential)
        found_blocks = re.findall(r"# Block (\d+\.\d+)", code)
        if found_blocks:
            numeric_blocks = [float(b) for b in found_blocks]
            if numeric_blocks != sorted(numeric_blocks):
                violations.append("Violation: Block hierarchy is out of order.")
                
        return violations

    def _attach_parents(self, tree: ast.AST):
        """Standard AST nodes don't have parents; we must inject them for scope resolution."""
        for node in ast.walk(tree):
            for child in ast.iter_child_nodes(node):
                child.parent = node

    def _extract_symbols(self, filename: str, tree: ast.AST, code: str):
        lines = code.splitlines()
        for node in ast.walk(tree):
            # Capture Top-Level Constants
            if isinstance(node, ast.Assign) and getattr(node, 'parent', None) == tree:
                if len(node.targets) == 1 and isinstance(node.targets[0], ast.Name):
                    sig = node.targets[0].id
                    self._symbol_table[sig] = self._create_symbol_entry(node, code, filename, lines)

            # Capture Classes and Functions (including methods)
            if isinstance(node, (ast.FunctionDef, ast.ClassDef)):
                sig = node.name
                parent = getattr(node, "parent", None)
                if isinstance(node, ast.FunctionDef) and isinstance(parent, ast.ClassDef):
                    sig = f"{parent.name}.{node.name}"
                
                entry = self._create_symbol_entry(node, code, filename, lines)
                entry["doc"] = ast.get_docstring(node) or ""
                self._symbol_table[sig] = entry

    def _create_symbol_entry(self, node: ast.AST, code: str, filename: str, lines: List[str]) -> Dict:
        """Helper to safely extract source segment even if lines are weird."""
        try:
            # Try native segment extraction
            segment = ast.get_source_segment(code, node) or ""
        except:
            # Fallback to manual slice
            segment = "\n".join(lines[node.lineno-1 : getattr(node, 'end_lineno', node.lineno)])
            
        return {
            "file": filename,
            "code": segment,
            "start_line": node.lineno,
            "end_line": getattr(node, 'end_lineno', node.lineno),
            "doc": ""
        }

    def find_symbol(self, signature: str) -> Optional[Dict]:
        return self._symbol_table.get(signature)

# === Block 5.0: Memory Modules (Optimized RAG & CAG) ===

class RAGEngine:
    """RAG Engine with optimized chunking for large-scale scripts."""
    # Updated for richer semantic context
    CHUNK_SIZE = 2048   
    CHUNK_OVERLAP = 512 

    def __init__(self):
        self.metadata: List[Dict] = []
        self.vectors: Optional[np.ndarray] = None
        self.dimension = 768
        self.load_index()

    def load_index(self):
        """Loads persistent vector index from the Vault."""
        if not NUMPY_AVAILABLE: return
        idx_path, meta_path = VAULT_PATHS["rag_index"], VAULT_PATHS["rag_metadata"]

        if idx_path.exists() and meta_path.exists():
            try:
                with MEMORY_LOCK:
                    self.vectors = np.load(idx_path)
                    self.metadata = json.loads(meta_path.read_text(encoding='utf-8'))
                    self.dimension = self.vectors.shape[1]
                log(f"RAG Index Loaded: {len(self.metadata)} chunks.", "DEBUG")
            except Exception as e:
                log(f"RAG reload failed: {e}", "ERROR")
        else:
            self.vectors, self.metadata = None, []

    def save_index(self):
        """Persists the index to the Vault."""
        if self.vectors is None or not NUMPY_AVAILABLE: return
        with MEMORY_LOCK:
            try:
                np.save(VAULT_PATHS["rag_index"], self.vectors)
                VAULT_PATHS["rag_metadata"].write_text(json.dumps(self.metadata, indent=2), encoding='utf-8')
            except Exception as e:
                log(f"RAG persistence failed: {e}", "ERROR")

    def _chunk_text(self, text: str) -> List[str]:
        """
        Semantic-aware chunking. 
        Splits text into 2048-char blocks while attempting to preserve line integrity.
        """
        chunks = []
        start = 0
        text_len = len(text)

        while start < text_len:
            end = start + self.CHUNK_SIZE
            if end < text_len:
                # Attempt to find the last newline within a 100-char buffer to avoid splitting lines
                last_newline = text.rfind('\n', end - 100, end)
                if last_newline != -1:
                    end = last_newline

            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end - self.CHUNK_OVERLAP
            # Safety to prevent infinite loops on weird files
            if start >= end: start = end + 1 
            
        return chunks
    def _chunk_by_ast(self, code_text: str) -> List[Dict[str, Any]]:
        """
        AST-aware chunking: One function/class = one chunk.
        Preserves semantic boundaries.
        Returns chunks with metadata (name, type, importance).
        """
        chunks = []
    
        try:
            tree = ast.parse(code_text)
        except SyntaxError:
            # Fallback to character-based chunking if syntax is broken
            log("AST chunking failed (syntax error), falling back to character chunking", "DEBUG")
            return [{"text": chunk, "type": "fallback", "name": "unknown", "importance": 0.5} 
                    for chunk in self._chunk_text(code_text)]
    
        # Extract top-level functions and classes
        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.ClassDef)):
                # Skip nested functions/methods (they're part of parent chunk)
                parent = getattr(node, 'parent', None)
                if parent and isinstance(parent, (ast.FunctionDef, ast.ClassDef)):
                    continue
            
                try:
                    chunk_text = ast.get_source_segment(code_text, node)
                    if not chunk_text:
                        continue
                
                    chunk_type = "function" if isinstance(node, ast.FunctionDef) else "class"
                    importance = self._calculate_importance(node, code_text)
                
                    chunks.append({
                        "text": chunk_text,
                        "type": chunk_type,
                        "name": node.name,
                        "importance": importance,
                        "start_line": node.lineno,
                        "end_line": getattr(node, 'end_lineno', node.lineno)
                    })
                except Exception as e:
                    log(f"Failed to extract chunk for {node.name}: {e}", "DEBUG")
                    continue
    
        # If no top-level functions/classes found, fallback to character chunking
        if not chunks:
            return [{"text": chunk, "type": "fallback", "name": "module", "importance": 0.5} 
                    for chunk in self._chunk_text(code_text)]
    
        return chunks


    def _calculate_importance(self, node: ast.AST, full_code: str) -> float:
        """
        Calculate chunk importance based on:
        - Size (longer = more important)
        - Docstring presence (documented = more important)
        - References (called often = more important)
        Returns: 0.0 to 1.0
        """
        importance = 0.5  # Base
    
        # Factor 1: Has docstring?
        if ast.get_docstring(node):
            importance += 0.2
    
        # Factor 2: Length (normalized to 0-0.3 range)
        try:
            code_segment = ast.get_source_segment(full_code, node) or ""
            lines = len(code_segment.splitlines())
            importance += min(0.3, lines / 100)  # Max bonus at 100+ lines
        except:
            pass
    
        # Factor 3: Referenced frequently? (count occurrences of name in code)
        if hasattr(node, 'name'):
            ref_count = full_code.count(node.name)
            importance += min(0.2, ref_count / 20)  # Max bonus at 20+ references
    
        return min(1.0, importance)  # Cap at 1.0

    def index_code_in_memory(self, code_text: str, signature: str, file_path: str = "accepted_code.py"):
        """Indexes symbols into the Vault using AST-aware semantic chunks."""
        if not NUMPY_AVAILABLE: return
    
        log(f"Deep-Indexing: {signature}...", "DEBUG")
        new_vectors, new_metadata = [], []

        # Use AST-aware chunking instead of character-based
        chunks = self._chunk_by_ast(code_text)
    
        for chunk_data in chunks:
            chunk_text = chunk_data["text"]
            vec = self._get_embedding(chunk_text)
            if vec is not None:
                new_vectors.append(vec)
                new_metadata.append({
                    "signature": signature,
                    "text": chunk_text,
                    "file": file_path,
                    "chunk_type": chunk_data["type"],
                    "chunk_name": chunk_data["name"],
                    "importance": chunk_data["importance"],
                    "timestamp": datetime.datetime.now().isoformat()
                })

        if new_vectors:
            with MEMORY_LOCK:
                new_vectors_np = np.array(new_vectors, dtype='float32')
                self.vectors = new_vectors_np if self.vectors is None else np.vstack([self.vectors, new_vectors_np])
                self.metadata.extend(new_metadata)
            self.save_index()
            log(f"Added {len(new_metadata)} AST-aware chunks.", "DEBUG")

    def _get_embedding(self, text: str) -> Optional[np.ndarray]:
        """Fetches embedding from local service."""
        try:
            resp = requests.post(OLLAMA_EMBEDDING_API, json={"model": EMBEDDING_MODEL, "prompt": text}, timeout=30)
            emb = resp.json().get('embedding', [])
            return np.array(emb, dtype='float32') if emb else None
        except Exception as e:
            log(f"Embedding error: {e}", "ERROR")
            return None

    def query(self, text: str, k: int = 3) -> List[Dict]:
        """
        Importance-weighted similarity search.
        Boosts results with higher importance scores.
        """
        if self.vectors is None or len(self.metadata) == 0: return []
    
        query_vec = self._get_embedding(text)
        if query_vec is None: return []

        # Cosine Similarity
        query_norm = query_vec / (np.linalg.norm(query_vec) + 1e-12)
        norms = np.linalg.norm(self.vectors, axis=1, keepdims=True)
        vecs_norm = self.vectors / np.where(norms == 0, 1e-12, norms)
        sims = np.dot(vecs_norm, query_norm)

        # Apply importance weighting
        importance_weights = np.array([m.get("importance", 0.5) for m in self.metadata])
        weighted_sims = sims * (0.7 + 0.3 * importance_weights)  # 70% similarity, 30% importance

        top_k_indices = np.argsort(weighted_sims)[::-1][:k]
    
        results = []
        for idx in top_k_indices:
            if weighted_sims[idx] > RAG_SIMILARITY_THRESHOLD and idx < len(self.metadata):
                chunk = self.metadata[idx].copy()
                chunk["similarity"] = float(sims[idx])
                chunk["weighted_score"] = float(weighted_sims[idx])
                results.append(chunk)
    
        return results

class CAGMemory:
    """CAG Memory for pattern-based learning."""
    def __init__(self):
        self.learned = load_learning_log()

    def add_success(self, signature: str, user_intent: str, code_delta: str, confidence: float = 0.9):
        """Logs high-confidence patterns."""
        self.learned.append({
            "id": datetime.datetime.now().strftime("%H%M%S"),
            "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "signature": signature,
            "user_intent": user_intent,
            "code_delta": code_delta[:2000], # Store larger snippets for patterns
            "confidence": confidence
        })
        save_learning_log(self.learned)

    def get_relevant(self, query: str, k: int = 2) -> List[Dict]:
        """Heuristic retrieval for learned patterns."""
        q = query.lower()
        scored = []
        for e in self.learned:
            score = (2 if q in e["user_intent"].lower() else 0) + (1 if q in e["signature"].lower() else 0)
            if score > 0: scored.append((score * e.get("confidence", 0.5), e))
        scored.sort(reverse=True, key=lambda x: x[0])
        return [e for _, e in scored[:k]]

# === Block 6.0: Agent Harness (LLM Manager and CAG Pipeline) ===

class MultiLLMManager:
    """Manages specialized LLM instances with Domain Memory injection."""
    def __init__(self):
        self.brains = {
            "coding": CODING_BRAIN,
            "reasoning": REASONING_BRAIN,
            "organizer": ORGANIZER_BRAIN
        }
        # Path to the stateful artifacts
        self.vibe_path = r"C:\Projects\ai-train\.vibe_index"

    def _apply_governance(self, brain: str) -> str:
        """
        Enforces hard limits on LLM behavior.
        Token limits prevent verbose responses.
        Output format requirements prevent chatter.
        """
        # Brain-specific token budgets
        token_limits = {
            "coding": 2048,      # Needs space for full functions
            "reasoning": 512,    # Verification should be brief
            "organizer": 64      # Classification is 1-10 tokens
        }
    
        max_tokens = token_limits.get(brain, 512)
    
        return (
            f"ROLE: {brain.upper()}\n"
            f"MAX_OUTPUT: {max_tokens} tokens (HARD LIMIT)\n"
            f"FORBIDDEN PATTERNS:\n"
            f"- Preambles ('I understand', 'Let me help', 'Sure', 'Certainly')\n"
            f"- Apologies ('Sorry', 'I apologize')\n"
            f"- Meta-commentary ('Here's what I did')\n"
            f"- Markdown formatting (use plain text or code only)\n"
            f"- Explanations unless explicitly requested\n"
            f"REQUIRED FORMAT:\n"
            f"- Code first, explanation never (unless asked)\n"
            f"- JSON for structured data\n"
            f"- Direct answers only\n"
            f"USER REQUEST: "
        )
    
    def _validate_output(self, brain: str, output: str, max_tokens: int) -> Dict[str, Any]:
        """
        Validates LLM output against governance rules.
        Returns validation report with pass/fail + violations.
        """
        violations = []
    
        # Check token count (rough estimate: 1 token ≈ 4 chars)
        estimated_tokens = len(output) // 4
        if estimated_tokens > max_tokens * 1.2:  # 20% grace period
            violations.append(f"Output too long: ~{estimated_tokens} tokens (limit: {max_tokens})")
    
        # Check for forbidden preambles
        forbidden_starts = [
            "I understand", "Let me help", "Sure", "Certainly", "Of course",
            "I'd be happy", "I can help", "I apologize", "Sorry"
        ]
        for phrase in forbidden_starts:
            if output.strip().lower().startswith(phrase.lower()):
                violations.append(f"Forbidden preamble detected: '{phrase}'")
                break
    
        # Check for markdown in non-code responses
        if brain != "coding" and ("```" in output or output.count("#") > 3):
            violations.append("Markdown formatting detected (should be plain text)")
    
        # Check for meta-commentary
        meta_phrases = ["here's what i", "i've created", "i've generated", "as you can see"]
        for phrase in meta_phrases:
            if phrase in output.lower():
                violations.append(f"Meta-commentary detected: '{phrase}'")
                break
    
        return {
            "valid": len(violations) == 0,
            "violations": violations,
            "estimated_tokens": estimated_tokens
        }
    
    def generate(self, brain: str, prompt: str, max_tokens: int = 4096, temperature: float = 0.2, system: Optional[str] = None) -> str:
        model = self.brains.get(brain, CODING_BRAIN)
    
        # Count input tokens
        prompt_tokens = count_tokens(prompt)
        if system:
            prompt_tokens += count_tokens(system)
    
        start_time = time.time()
    
        payload = {
            "model": model,
            "prompt": prompt,
            "system": system if system else self._apply_governance(brain),
            "stream": False,
            "options": {
                "num_predict": max_tokens, 
                "temperature": temperature,
                "stop": ["User:", "Assistant:", "```"]
            }
        }
    
        try:
            resp = requests.post(OLLAMA_API, json=payload, timeout=120)
            resp.raise_for_status()
            output = resp.json().get("response", "").strip()
        
            # Metrics
            response_tokens = count_tokens(output)
            duration = time.time() - start_time
            log_metrics(brain, prompt_tokens, response_tokens, duration, "OK")
        
            # Validate governance compliance
            validation = self._validate_output(brain, output, max_tokens)
            if not validation["valid"]:
                log(f"[GOVERNANCE VIOLATION] {brain}: {validation['violations']}", "WARN")
        
            return output
        
        except Exception as e:
            duration = time.time() - start_time
            log_metrics(brain, prompt_tokens, 0, duration, "ERROR")
            log(f"LLM Error ({brain}): {e}", "ERROR")
            return f"[LLM ERROR] {e}"

    def stream_generate(self, brain: str, prompt: str, system: Optional[str] = None, max_tokens: int = 512, temperature: float = 0.3):
        model = self.brains.get(brain, CODING_BRAIN)
        payload = {
            "model": model,
            "prompt": prompt,
            "system": system if system else self._apply_governance(brain),  # ← USE CUSTOM SYSTEM IF PROVIDED
            "stream": True,
            "options": {"num_predict": max_tokens, "temperature": temperature}
        }
        try:
            resp = requests.post(OLLAMA_API, json=payload, stream=True, timeout=300)
            for line in resp.iter_lines():
                if line:
                    chunk = json.loads(line.decode("utf-8"))
                    if "response" in chunk: yield chunk["response"]
                    if chunk.get("done"): break
        except Exception as e:
            log(f"Stream Error: {e}", "ERROR")
            yield f"[LLM ERROR] {e}"

class CAGPipeline:
    """Orchestrates modifications using RAG/CAG memory and the Domain Manifest."""
    def __init__(self, llm: MultiLLMManager, rag: Optional[RAGEngine], analyzer: PythonAnalyzer, memory: CAGMemory):
        self.llm, self.rag, self.analyzer, self.memory = llm, rag, analyzer, memory

    def run_modify_target(self, user_instruction: str, target_signature: str) -> Dict:
        conv = get_conversation()
        code_to_analyze = conv.get("current_code", "")
        if not code_to_analyze:
            return {"error": "No code in memory.", "target": target_signature}

        self.analyzer.analyze_in_memory(code_to_analyze)
        symbol = self.analyzer.find_symbol(target_signature)
        if not symbol:
            return {"error": f"Symbol '{target_signature}' not found.", "target": target_signature}

        # 1. Domain Memory Retrieval (RAG/CAG)
        rag_results = self.rag.query(user_instruction + " " + target_signature, k=3) if self.rag else []
        learned = self.memory.get_relevant(user_instruction, k=2)

        rag_context = "\n".join([f"[{r.get('signature', 'N/A')}]: {r.get('text', '')[:200]}" for r in rag_results])
        learned_context = "\n".join([f"[PATTERN {i+1}]: {p.get('user_intent', '')} -> {p.get('code_delta', '')[:100]}" for i, p in enumerate(learned)])

        # 2. Execution (The Coding Actor)
        prompt = f"""You are the Coding Brain. 
TARGET: {target_signature}
CURRENT CODE: {symbol.get('code', '')}
RAG CONTEXT: {rag_context or 'None'}
LEARNED PATTERNS: {learned_context or 'None'}
INSTRUCTION: {user_instruction}
Return ONLY the revised code block."""

        revised_code = self.llm.generate("coding", prompt)

        # 3. Verification (The Audit/Reasoning Brain)
        verify_prompt = f"Review this change for {target_signature}:\n{revised_code}\nCheck syntax and constitutional logic."
        verification = self.llm.generate("reasoning", verify_prompt, max_tokens=1024, temperature=0.1)

        return {
            "revised_code": revised_code,
            "verification": verification,
            "target": target_signature,
            "status": "success"
        }

def save_cag_and_rag(target: str, user_instruction: str, accepted_code: str):
    """Persists work to the .vibe_index Domain Memory."""
    if not accepted_code.strip(): return
    
    # Save to CAG Pattern Memory
    if 'cag_memory' in globals() and cag_memory:
        try:
            cag_memory.add_success(target, user_instruction, accepted_code)
            log(f"CAG memory saved: {target}", "PERSISTENCE")
        except Exception as e:
            log(f"CAG Save failed: {e}", "ERROR")

    # Save to RAG Vector Index
    if 'rag_engine' in globals() and rag_engine and rag_engine.vectors is not None:
        try:
            rag_engine.index_code_in_memory(accepted_code, signature=target)
            rag_engine.save_index()
            log(f"RAG indexed: {target}", "PERSISTENCE")
        except Exception as e:
            log(f"RAG Save failed: {e}", "ERROR")

# === Block 7.0: Intent and Initialization Agents ===

def detect_intent(user_input: str) -> Tuple[str, Optional[str]]:
    """Detects user intent (e.g., modify-target, general) and validates the target signature."""
    global llm_manager, analyzer
    if not llm_manager:
        return "general", None

    prompt = f"""Classify this query into ONE keyword: modify-target, review, fix, explain, general.
If modify-target, extract the target signature (e.g., ClassName.method_name or function_name).
The 'index' command is not supported in the Blank Workspace mode.
Format: KEYWORD or KEYWORD:TARGET
Query: {user_input}"""
    
    # Using organizer brain as per original working logic
    response = llm_manager.generate("organizer", prompt, max_tokens=50)

    stripped_response = response.strip()
    if not stripped_response:
        return "general", None

    parts = stripped_response.split(":", 1)
    intent = parts[0].strip().lower()
    target = parts[1].strip() if len(parts) > 1 else None

    if intent not in ["modify-target", "review", "fix", "explain", "general"]:
        intent = "general"

    if intent == "modify-target" and target and analyzer:
        conv = get_conversation()
        # Ensure we are looking at the most recent code in the context window
        analyzer.analyze_in_memory(conv.get("current_code", ""))
        if not analyzer.find_symbol(target):
            log(f"Detected target '{target}' not in current code. Downgrading to 'general'.", "WARN")
            return "general", None

    return intent, target


class InitializerAgent:
    """Consumes a high-level project description and generates the authoritative feature_list.json."""
    FEATURE_TEMPLATE = {
        "id": "TBD",
        "goal": "TBD",
        "status": "new",
        "history": []
    }

    def __init__(self, llm_manager: MultiLLMManager, feature_manager: FeatureListManager):
        self.llm = llm_manager
        self.feature_manager = feature_manager

    def generate_feature_list(self, project_description: str) -> Tuple[bool, str]:
        """Uses the organizer brain to decompose the project into testable features."""
        system_prompt = f"""You are the Project Stage Manager. Your task is to decompose a high-level project description into a list of atomic, sequential, and testable features.
Return a single JSON object. Do not include markdown or explanations.
The JSON object must have a single key 'features', containing a list of feature objects.
Each feature object must follow this template:
{json.dumps(self.FEATURE_TEMPLATE, indent=2)}
- 'id' must be unique (e.g., 'A001').
- 'goal' is the executable instruction for the Coding Agent.
- 'status' is always 'new'.
- 'history' is always an empty list [].
Project Description: {project_description}"""

        log("Sending project plan to ORGANIZER_BRAIN...")
        raw_response = self.llm.generate(
            "organizer",
            project_description,
            system=system_prompt,
            temperature=0.5,
            max_tokens=2048
        )

        raw_text = raw_response.strip()

        try:
            # Attempt 1: Raw JSON
            data = json.loads(raw_text)
        except json.JSONDecodeError:
            try:
                # Attempt 2: Extract from markdown fences (Using helper from Block 1/2)
                extracted = extract_code(raw_response)
                if not extracted:
                    raise ValueError("No JSON object found in LLM response.")
                data = json.loads(extracted)
            except Exception as e:
                log(f"LLM failed to return valid JSON. Error: {e}", "ERROR")
                return False, f"LLM failed to return valid JSON. Error: {e}"

        if not isinstance(data, dict) or "features" not in data or not isinstance(data["features"], list):
            log("LLM returned invalid JSON structure.", "ERROR")
            return False, "LLM returned invalid structure."

        features = data["features"]
        if not features:
            return False, "LLM generated an empty feature list."

        # Assign IDs if missing to ensure Domain Memory is indexed properly
        for i, f in enumerate(features):
            if not f.get("id") or not f["id"].strip() or f["id"] == "TBD":
                f["id"] = f"A{i+1:03d}"
                log(f"Generated missing ID: {f['id']}", "DEBUG")

        # Persistence to C:\Projects\ai-train\.vibe_index\feature_list.json
        self.feature_manager.data = data
        self.feature_manager.save()
        log(f"Successfully generated and saved {len(features)} features.")

        return True, f"Project plan decomposed into {len(features)} features. Boot Ritual starting..."
    
# === Block 8.0: Python Validation Pipeline ===
# ARCHITECTURE: Non-blocking feedback system.
# Code ALWAYS flows to the UI; diagnostics provide "Lived Context" for the next iteration.

def run_python_validation(code_text: str) -> Dict[str, Any]:
    """
    Multi-layer validation using Python AST + PowerShell + Heuristics.
    Returns comprehensive diagnostic report.
    """
    if not ENABLE_PS_VALIDATION:
        return {"syntax": {"status": "skipped"}, "lint": {"warnings": 0}, "strictmode": {"risks": 0}}
    
    results = {
        "syntax": {"status": "ok", "issues": []},
        "style": {"warnings": 0, "issues": []},
        "lint": {"warnings": 0, "issues": []},
        "strictmode": {"risks": 0, "issues": []},
        "governance": {"status": "ok", "issues": []}
    }

    # === Layer 1: Python AST Syntax Check ===
    try:
        compile(code_text, "<memory_code>", "exec")
    except SyntaxError as e:
        results["syntax"]["status"] = "error"
        results["syntax"]["issues"].append(f"Line {e.lineno}: {e.msg}")
        return results  # Stop if syntax is broken

    # === Layer 2: PowerShell Indentation + Style Check ===
    if ENABLE_PS_VALIDATION:
        ps_results = _run_powershell_validation(code_text)
        if ps_results["indentation_errors"]:
            results["style"]["warnings"] += len(ps_results["indentation_errors"])
            results["style"]["issues"].extend(ps_results["indentation_errors"])
        if ps_results["style_warnings"]:
            results["style"]["warnings"] += len(ps_results["style_warnings"])
            results["style"]["issues"].extend(ps_results["style_warnings"])

    # === Layer 3: Heuristic Lint Checks ===
    if "import *" in code_text:
        results["lint"]["warnings"] += 1
        results["lint"]["issues"].append("Wildcard import detected; breaks explicit domain mapping.")
    
    # Check for long lines (PEP 8: max 79-88 chars)
    for i, line in enumerate(code_text.splitlines(), 1):
        if len(line) > 100:
            results["lint"]["warnings"] += 1
            results["lint"]["issues"].append(f"Line {i}: Exceeds 100 characters ({len(line)} chars)")
    
    # === Layer 4: Security StrictMode ===
    if re.search(r"\beval\(", code_text) or re.search(r"\bexec\(", code_text):
        results["strictmode"]["risks"] += 1
        results["strictmode"]["issues"].append("Dynamic execution (eval/exec) detected; potential security risk.")
    
    # Check for hardcoded secrets
    secret_patterns = [
        r"password\s*=\s*['\"]",
        r"api_key\s*=\s*['\"]",
        r"token\s*=\s*['\"]",
        r"secret\s*=\s*['\"]"
    ]
    for pattern in secret_patterns:
        if re.search(pattern, code_text, re.IGNORECASE):
            results["strictmode"]["risks"] += 1
            results["strictmode"]["issues"].append(f"Hardcoded credential detected: {pattern}")
    
    # === Layer 5: Constitutional Governance ===
    if "import " in code_text and "app = Flask" in code_text:
        import_idx = code_text.find("import ")
        flask_idx = code_text.find("app = Flask")
        if flask_idx < import_idx:
            results["governance"]["status"] = "warning"
            results["governance"]["issues"].append("Block Hierarchy Violation: Imports must precede UI initialization.")

    return results


def _run_powershell_validation(code_text: str) -> Dict[str, List[str]]:
    """
    Executes PowerShell script to validate Python indentation and style.
    Returns indentation errors and style warnings.
    """
    results = {
        "indentation_errors": [],
        "style_warnings": []
    }
    
    # PowerShell script that checks indentation consistency
    ps_script = f"""
$code = @'
{code_text}
'@

$lines = $code -split "`n"
$indentLevels = @()
$inconsistencies = @()

for ($i = 0; $i -lt $lines.Length; $i++) {{
    $line = $lines[$i]
    if ($line -match '^(\\s+)') {{
        $indent = $matches[1]
        $spaces = ($indent -replace "`t", "    ").Length
        
        # Check for mixed tabs/spaces
        if ($indent -match "`t" -and $indent -match " ") {{
            $inconsistencies += "Line $($i+1): Mixed tabs and spaces"
        }}
        
        # Check for non-4-space indentation
        if ($spaces % 4 -ne 0) {{
            $inconsistencies += "Line $($i+1): Indentation is not multiple of 4 spaces ($spaces spaces)"
        }}
    }}
}}

# Check for trailing whitespace
for ($i = 0; $i -lt $lines.Length; $i++) {{
    if ($lines[$i] -match '\\s+$') {{
        $inconsistencies += "Line $($i+1): Trailing whitespace"
    }}
}}

$inconsistencies -join "`n"
"""
    
    try:
        # Write temp file to avoid command-line length limits
        ps_file = VAULT_DIR / "temp_validation.ps1"
        ps_file.write_text(ps_script, encoding='utf-8')
        
        result = subprocess.run(
            ["powershell", "-ExecutionPolicy", "Bypass", "-File", str(ps_file)],
            capture_output=True,
            text=True,
            timeout=PS_TIMEOUT
        )
        
        ps_file.unlink()  # Clean up
        
        if result.stdout.strip():
            for line in result.stdout.strip().split('\n'):
                if line.strip():
                    if "Mixed tabs" in line or "not multiple of 4" in line:
                        results["indentation_errors"].append(line.strip())
                    else:
                        results["style_warnings"].append(line.strip())
        
    except subprocess.TimeoutExpired:
        log("PowerShell validation timeout", "WARN")
    except Exception as e:
        log(f"PowerShell validation error: {e}", "ERROR")
    
    return results

def format_validation_report(validation_result: Dict[str, Any]) -> str:
    """Enhanced validation report with PowerShell integration."""
    if validation_result.get("syntax", {}).get("status") == "skipped":
        return "[Validation disabled]"
    
    lines = []
    
    # Syntax
    syntax = validation_result.get("syntax", {})
    if syntax.get("status") == "ok":
        lines.append("✓ Syntax: Clean")
    else:
        lines.append("✗ Syntax: Errors detected")
        for issue in syntax.get("issues", []): 
            lines.append(f"  - {issue}")
    
    # Style (PowerShell)
    style = validation_result.get("style", {})
    if style.get("warnings", 0) > 0:
        lines.append(f"⚠ Style: {style['warnings']} warning(s)")
        for issue in style.get("issues", [])[:3]:  # Limit to first 3
            lines.append(f"  - {issue}")
        if len(style.get("issues", [])) > 3:
            lines.append(f"  ... and {len(style['issues']) - 3} more")
    else:
        lines.append("✓ Style: Clean")
    
    # Lint
    lint = validation_result.get("lint", {})
    if lint.get("warnings", 0) > 0:
        lines.append(f"⚠ Lint: {lint['warnings']} warning(s)")
        for issue in lint.get("issues", [])[:3]:
            lines.append(f"  - {issue}")
    
    # Governance
    gov = validation_result.get("governance", {})
    if gov.get("status") != "ok":
        lines.append("⚠ Governance: Pattern Mismatch")
        for issue in gov.get("issues", []):
            lines.append(f"  - {issue}")
    
    # StrictMode
    strict = validation_result.get("strictmode", {})
    if strict.get("risks", 0) > 0:
        lines.append(f"StrictMode: {strict['risks']} SECURITY RISK(S)")
        for issue in strict.get("issues", []):
            lines.append(f"  - {issue}")
    
    # Summary
    if all([
        syntax.get("status") == "ok",
        style.get("warnings", 0) == 0,
        lint.get("warnings", 0) == 0,
        strict.get("risks", 0) == 0,
        gov.get("status") == "ok"
    ]):
        return "✓ Code Quality: Maverick-Compliant (All checks passed)"
        
    return "\n".join(lines)

# === Block 9.0: Flask Routes ===

@app.route("/")
def index():
    """Serves the main UI."""
    return render_template("index.html")

@app.route("/favicon.ico")
def favicon():
    """Prevents 404 for favicon requests."""
    return '', 204

@app.route("/api/features", methods=["GET"])
def api_get_features():
    """Returns the current feature list for the UI from Domain Memory."""
    global feature_manager
    if not feature_manager:
        feature_manager = FeatureListManager()
    
    return jsonify({
        "status": "success",
        "features": feature_manager.data.get("features", [])
    }), 200

@app.route("/api/diagnostic/llms", methods=["GET"])
def diagnostic_llms():
    """Diagnostic: Verify the 'Nervous System' (all 3 LLMs) is reachable."""
    global llm_manager
    if not llm_manager:
        return jsonify({"status": "error", "message": "MultiLLMManager not initialized"}), 500
    
    results = {}
    test_prompt = "Reply with: OK"
    
    for brain_name, cfg in BRAIN_CONFIG.items():
        try:
            response = llm_manager.generate(brain_name, test_prompt)
            results[brain_name] = {
                "model": cfg["model"],
                "status": "reachable" if response and not response.startswith("[LLM ERROR]") else "failed",
                "response": response[:50]
            }
        except Exception as e:
            results[brain_name] = {
                "model": cfg["model"], "status": "error", "error": str(e)
            }
    
    return jsonify({"llms": results}), 200

@app.route("/api/metrics", methods=["GET"])
def api_metrics():
    """Returns aggregated LLM usage metrics."""
    # This requires storing metrics in memory or a file
    # For now, return a placeholder structure
    return jsonify({
        "status": "success",
        "message": "Metrics collection active. Check logs for [METRICS] entries.",
        "note": "Full metrics dashboard coming in Phase 5"
    }), 200

@app.route("/api/init", methods=["POST"])
def api_init():
    """Initializes the project and bootstraps the Domain Memory Factory."""
    global feature_manager, llm_manager
    if not feature_manager:
        feature_manager = FeatureListManager()
    if not llm_manager:
        llm_manager = MultiLLMManager()
    
    try:
        project_description = request.json.get("description", "").strip()
    except Exception:
        return jsonify({"status": "error", "message": "Invalid JSON input."}), 400
    
    if not project_description:
        return jsonify({"status": "error", "message": "Missing project description."}), 400
    
    initializer = InitializerAgent(llm_manager, feature_manager)
    success, message = initializer.generate_feature_list(project_description)
    
    if success:
        conv = get_conversation()
        conv["scratchpad"] = {}
        conv["current_code"] = ""
        next_feature = feature_manager.get_next_failing_feature()
        if next_feature:
            conv["current_feature_id"] = next_feature["id"]
            conv["scratchpad"][next_feature["id"]] = {"code_to_propose": ""}
            save_state()
            next_goal = next_feature.get("goal", "No goal found.")
        else:
            next_goal = "Initialization complete."
            conv["current_feature_id"] = None
            save_state()
        
        return jsonify({
            "status": "success",
            "message": message,
            "next_feature_goal": next_goal,
            "current_feature_id": conv.get("current_feature_id")
        }), 200
    return jsonify({"status": "error", "message": message}), 500

@app.route("/api/chat", methods=["POST"])
def chat():
    """Main SSE endpoint. Supports both idle chat and feature-locked modes."""
    global feature_manager, llm_manager, cag_pipeline
    conv = get_conversation()
    
    fid = conv.get("current_feature_id")
    feature = feature_manager.get_feature_by_id(fid) if fid and feature_manager else None

    def sse_error_response(message: str, status_code: int = 400):
        error_data = json.dumps({"content": message, "type": "error", "done": True})
        return Response(f"data: {error_data}\n\n", mimetype="text/event-stream", status=status_code)

    try:
        data = request.json
        user_input = data.get("message", "").strip()
        editor_code = data.get("editor_code", "").strip()
    except Exception:
        return sse_error_response("Invalid JSON input.")

    if not user_input:
        return sse_error_response("Message cannot be empty.")

    # Initialize scratchpad for feature if needed
    if fid and fid not in conv["scratchpad"]:
        conv["scratchpad"][fid] = {"code_to_propose": ""}

    working_code = editor_code if editor_code else (conv["scratchpad"].get(fid, {}).get("code_to_propose", "") if fid else "")
    if editor_code and fid:
        conv["scratchpad"][fid]["code_to_propose"] = editor_code

    intent, target = detect_intent(user_input)

    def generate_response():
        feature_context = f"\nGoal: {feature['goal']}" if feature else ""

        # === CAG Modify-Target Path ===
        if intent == "modify-target" and target and cag_pipeline:
            try:
                start_data = json.dumps({'content': f'**Targeted modification for {target}...**', 'type': 'narrative'})
                yield f"data: {start_data}\n\n"

                result = cag_pipeline.run_modify_target(user_input, target)
                revised_code = extract_code(result.get("revised_code", ""))
                verification = result.get("verification", "")

                # === Hallucination detection for CAG path ===
                hallucination_report = detect_hallucinations(revised_code, working_code)

                if not hallucination_report["safe_to_persist"]:
                    warning_msg = "\n⚠️ Code Quality Warning (Targeted Edit):\n"
                    warning_msg += f"Confidence: {hallucination_report['confidence']:.0%}\n"
                    for issue in hallucination_report["issues"]:
                        warning_msg += f"- {issue}\n"
                    warning_msg += "\nRecommendation: Review carefully before accepting."

                    warn_data = json.dumps({'content': warning_msg, 'type': 'narrative'})
                    yield f"data: {warn_data}\n\n"

                # Persist only if hallucination check passes
                if fid and hallucination_report["safe_to_persist"]:
                    conv["scratchpad"][fid] = {
                        "code_to_propose": revised_code,
                        "target_symbol": target,
                        "user_instruction": user_input
                    }
                    save_state()

                msg_data = json.dumps({'content': f'**Reasoning Brain:**\n{verification}', 'type': 'narrative'})
                yield f"data: {msg_data}\n\n"

                val_data = json.dumps({'content': '\n**Validation Report:**\n[PASSED] Syntax Check', 'type': 'narrative'})
                yield f"data: {val_data}\n\n"

                code_data = json.dumps({'content': revised_code, 'type': 'code', 'feature_id': fid, 'target': target})
                yield f"data: {code_data}\n\n"

            except Exception as e:
                err_payload = json.dumps({'content': f'[FATAL] CAG failed: {e}', 'type': 'error', 'done': True})
                yield f"data: {err_payload}\n\n"
                return

        # === Freeform / Idle Coding Path ===
        else:
            system_prompt = f"Source of Truth Code:\n```python\n{working_code}\n```{feature_context}"
            active_data = json.dumps({'content': '**Coding Agent Active...**\n', 'type': 'narrative'})
            yield f"data: {active_data}\n\n"

            full_response = ""
            try:
                for chunk in llm_manager.stream_generate("coding", user_input, system=system_prompt, max_tokens=3072, temperature=0.4):
                    if chunk and not chunk.startswith("[LLM ERROR]"):
                        full_response += chunk
                        chunk_data = json.dumps({'content': chunk, 'type': 'stream'})
                        yield f"data: {chunk_data}\n\n"

                if full_response:
                    code_block = extract_code(full_response)
                    if code_block:
                        hallucination_report = detect_hallucinations(code_block, working_code)

                        if not hallucination_report["safe_to_persist"]:
                            warning_msg = "\n⚠️ Code Quality Warning:\n"
                            warning_msg += f"Confidence: {hallucination_report['confidence']:.0%}\n"
                            for issue in hallucination_report["issues"]:
                                warning_msg += f"- {issue}\n"
                            warning_msg += "\nRecommendation: Review carefully before accepting."
                            warn_data = json.dumps({'content': warning_msg, 'type': 'narrative'})
                            yield f"data: {warn_data}\n\n"

                        validation_result = run_python_validation(code_block)
                        validation_report = format_validation_report(validation_result)

                        val_data = json.dumps({
                            'content': f'\n**Validation Report:**\n{validation_report}',
                            'type': 'narrative'
                        })
                        yield f"data: {val_data}\n\n"

                        if fid and hallucination_report["safe_to_persist"]:
                            conv["scratchpad"][fid]["code_to_propose"] = code_block
                            save_state()

                        final_code = json.dumps({'content': code_block, 'type': 'code', 'feature_id': fid})
                        yield f"data: {final_code}\n\n"

            except Exception as e:
                stream_err = json.dumps({'content': f'[ERROR] Streaming failed: {e}', 'type': 'error'})
                yield f"data: {stream_err}\n\n"

        done_payload = json.dumps({'done': True})
        yield f"data: {done_payload}\n\n"

    return Response(generate_response(), mimetype="text/event-stream", 
                    headers={"Cache-Control": "no-cache", "X-Accel-Buffering": "no"})

# === Block 10.0: Application Initialization ===

def initialize_core_managers():
    """Explicitly initializes all core managers and anchors them to the Vibe Index."""
    global feature_manager, llm_manager, rag_engine, analyzer, cag_memory, cag_pipeline
    
    log("Initializing core managers...")
    
    if not feature_manager:
        feature_manager = FeatureListManager()
        log("✓ FeatureListManager initialized", "DEBUG")
    
    if not llm_manager:
        llm_manager = MultiLLMManager()
        log("✓ MultiLLMManager initialized", "DEBUG")
    
    if not rag_engine and NUMPY_AVAILABLE:
        rag_engine = RAGEngine()
        log("✓ RAGEngine initialized", "DEBUG")
    
    if not cag_memory:
        cag_memory = CAGMemory()
        log("✓ CAGMemory initialized", "DEBUG")
    
    if not analyzer:
        analyzer = PythonAnalyzer()
        log("✓ PythonAnalyzer initialized", "DEBUG")
    
    if not cag_pipeline and llm_manager and analyzer and cag_memory:
        cag_pipeline = CAGPipeline(llm_manager, rag_engine, analyzer, cag_memory)
        log("✓ CAGPipeline initialized", "DEBUG")


# === Block 11.0: Main Entry Point ===

if __name__ == "__main__":
    log("Starting VIBE Flask server...")
    
    # Single initialization sequence
    ensure_memory_files()
    initialize_core_managers()
    
    # Start server
    app.run(host="0.0.0.0", port=5000, debug=True)